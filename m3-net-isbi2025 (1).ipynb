{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9459147,"sourceType":"datasetVersion","datasetId":5750489},{"sourceId":9459857,"sourceType":"datasetVersion","datasetId":5751027},{"sourceId":9468196,"sourceType":"datasetVersion","datasetId":5757381},{"sourceId":9938099,"sourceType":"datasetVersion","datasetId":6109806},{"sourceId":10143494,"sourceType":"datasetVersion","datasetId":6260952},{"sourceId":11157461,"sourceType":"datasetVersion","datasetId":6961613},{"sourceId":170147,"sourceType":"modelInstanceVersion","modelInstanceId":144764,"modelId":167321},{"sourceId":170198,"sourceType":"modelInstanceVersion","modelInstanceId":144808,"modelId":167364},{"sourceId":171264,"sourceType":"modelInstanceVersion","modelInstanceId":145756,"modelId":168326},{"sourceId":193267,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164831,"modelId":187159},{"sourceId":193356,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164908,"modelId":187233},{"sourceId":194564,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":165862,"modelId":188198}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils import normalize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Conv2DTranspose, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization, Activation, MaxPool2D, Multiply, GlobalAveragePooling2D, Reshape, Dense, Lambda\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom keras.losses import binary_crossentropy\nfrom tensorflow.keras.applications import VGG16\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve\nimport seaborn as sns\nfrom skimage.restoration import denoise_nl_means, estimate_sigma\nfrom skimage import img_as_ubyte, img_as_float, io\nfrom scipy.ndimage import binary_opening\n\ndef load_and_preprocess_data(folder_path,imagesf,maskf):\n    # Load images\n    image_names = glob.glob(os.path.join(folder_path, imagesf, \"*.tif\"))\n    image_names.sort()\n    images = []\n    for image_path in image_names:\n        image = cv2.imread(image_path, 1)\n        if image is not None:\n            images.append(image)\n\n    # Load masks\n    mask_names = glob.glob(os.path.join(folder_path, maskf, \"*.png\"))\n    mask_names.sort()\n    masks = []\n    for mask_path in mask_names:\n        mask = cv2.imread(mask_path, 1)\n        if mask is not None:\n            gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n            masks.append(gray_mask)\n\n    return images, masks\n\n\ndef create_multiscale_patches(image, large_patch_size, medium_patch_size, small_patch_size, stride):\n    \"\"\"\n    Splits an image into multi-scale patches of given sizes with specified stride.\n\n    :param image: Input image to split into patches.\n    :param large_patch_size: Size of each large patch.\n    :param medium_patch_size: Size of each medium patch.\n    :param small_patch_size: Size of each small patch.\n    :param stride: Number of pixels to move in both horizontal and vertical directions for the next patch.\n    :return: Lists of large, medium, and small patches.\n    \"\"\"\n    large_patches = []\n    medium_patches = []\n    small_patches = []\n\n    h, w = image.shape[:2]\n\n    for y in range(0, h - large_patch_size + 1, stride):\n        for x in range(0, w - large_patch_size + 1, stride):\n            large_patch = image[y:y + large_patch_size, x:x + large_patch_size]\n            large_patches.append(large_patch)\n\n            # Calculate center for medium patch within the large patch\n            m_x_center = x + (large_patch_size - medium_patch_size) // 2\n            m_y_center = y + (large_patch_size - medium_patch_size) // 2\n\n            if m_x_center + medium_patch_size <= w and m_y_center + medium_patch_size <= h:\n                medium_patch = image[m_y_center:m_y_center + medium_patch_size, m_x_center:m_x_center + medium_patch_size]\n                medium_patches.append(medium_patch)\n\n                # Calculate center for small patch within the medium patch\n                l_x_center = m_x_center + (medium_patch_size - small_patch_size) // 2\n                l_y_center = m_y_center + (medium_patch_size - small_patch_size) // 2\n\n                if l_x_center + small_patch_size <= w and l_y_center + small_patch_size <= h:\n                    small_patch = image[l_y_center:l_y_center + small_patch_size, l_x_center:l_x_center + small_patch_size]\n                    small_patches.append(small_patch)\n\n    return large_patches, medium_patches, small_patches\n\n\ndef resize_patches(patches, target_size, is_mask=False):\n    \"\"\"\n    Resize patches to the target size. Uses nearest-neighbor interpolation for masks.\n\n    :param patches: List of patches to resize.\n    :param target_size: Target size for resizing (tuple of width, height).\n    :param is_mask: Boolean indicating if the patches are masks. Defaults to False.\n    :return: Resized patches as a numpy array.\n    \"\"\"\n    interpolation_method = cv2.INTER_NEAREST if is_mask else cv2.INTER_LINEAR\n    resized_patches = [cv2.resize(patch, (target_size, target_size), interpolation=interpolation_method) for patch in patches]\n    return np.array(resized_patches)\n\ndef encode_mask(mask_patches):\n    labelencoder = LabelEncoder()\n    n, h, w = mask_patches.shape  \n    mask_dataset_reshaped = mask_patches.reshape(-1, 1)\n    mask_dataset_reshaped_encoded = labelencoder.fit_transform(mask_dataset_reshaped.ravel())\n    mask_dataset_encoded = mask_dataset_reshaped_encoded.reshape(n, h, w)\n    mask_dataset_encoded = np.expand_dims(mask_dataset_encoded, axis=3)\n    return mask_dataset_encoded\n\n\ndef categorize_and_reshape_masks(y, n_classes):\n    # Convert masks to one-hot encoded format\n    y_masks_cat = to_categorical(y, num_classes=n_classes)\n    # Reshape the one-hot encoded masks to the desired shape\n    y_cat = y_masks_cat.reshape((y.shape[0], y.shape[1], y.shape[2], n_classes))\n\n    return y_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess training data\ntrain_folder = \"/kaggle/input/monusac-public/MoNuSac\"\ntrain_images, train_masks = load_and_preprocess_data(train_folder,\"images\",\"binary_masks\")\n\n\n\n# Define patch sizes and stride\nlarge_patch_size = 256\nmedium_patch_size = 192\nsmall_patch_size = 128\nstride = 256\n\n# Create multi-scale patches for training data\ntrain_image_large_patches, train_image_medium_patches, train_image_small_patches = [], [], []\ntrain_mask_large_patches = []\n\nfor image, mask in zip(train_images, train_masks):\n    large_patches, medium_patches, small_patches = create_multiscale_patches(image, large_patch_size, medium_patch_size, small_patch_size, stride)\n    train_image_large_patches.extend(large_patches)\n    train_image_medium_patches.extend(medium_patches)\n    train_image_small_patches.extend(small_patches)\n\n    large_patches, _, _ = create_multiscale_patches(mask, large_patch_size, medium_patch_size, small_patch_size, stride)\n    train_mask_large_patches.extend(large_patches)\n\n\n\n# Convert lists to numpy arrays\ntrain_image_large_patches = np.array(train_image_large_patches)\ntrain_image_medium_patches = np.array(train_image_medium_patches)\ntrain_image_small_patches = np.array(train_image_small_patches)\ntrain_mask_large_patches = np.array(train_mask_large_patches)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resize patches\ntrain_image_large_patches_resized = train_image_large_patches  # No need to resize\ntrain_image_medium_patches_resized = resize_patches(train_image_medium_patches, large_patch_size)\ntrain_image_small_patches_resized = resize_patches(train_image_small_patches, large_patch_size)\ntrain_mask_large_patches_resized = train_mask_large_patches  # No need to resize\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Image small patch shape is: \", train_image_small_patches_resized.shape)\nprint(\"Image Medium patch shape is: \", train_image_medium_patches_resized.shape)\nprint(\"Image Large patch shape is: \", train_image_large_patches_resized.shape, \"and Mask Large patch shape is: \", train_mask_large_patches_resized.shape)\n\nprint(\"Max pixel value in image is: \", train_image_large_patches_resized.max())\nunique_labels = np.unique(train_mask_large_patches_resized)\nprint(\"Labels in the mask are : \", unique_labels)\nnum_classes = len(unique_labels)\nprint(\"Total Classes in the mask are : \", num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize images\ntrain_image_large_patches_resized = train_image_large_patches_resized / 255.\ntrain_image_medium_patches_resized = train_image_medium_patches_resized / 255.\ntrain_image_small_patches_resized = train_image_small_patches_resized / 255.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode masks\ntrain_mask_large_dataset_encoded = encode_mask(train_mask_large_patches_resized)\n\n\n# Get number of classes\nunique_labels = np.unique(train_mask_large_dataset_encoded)\nnum_classes = len(unique_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Categorize and reshape masks\ntrain_large_mask_cat = categorize_and_reshape_masks(train_mask_large_dataset_encoded, num_classes)\n\n\n# Split training data into training and validation sets\nX_large_train, X_large_val, X_medium_train, X_medium_val, X_small_train, X_small_val, y_train, y_val = train_test_split(\n    train_image_large_patches_resized, train_image_medium_patches_resized, train_image_small_patches_resized, train_large_mask_cat, test_size=0.2, random_state=42\n)\n\n# Print shapes\nprint(\"Training data shapes:\")\nprint(\"X_large_train:\", X_large_train.shape)\nprint(\"X_medium_train:\", X_medium_train.shape)\nprint(\"X_small_train:\", X_small_train.shape)\nprint(\"y_train:\", y_train.shape)\n\nprint(\"\\nValidation data shapes:\")\nprint(\"X_large_val:\", X_large_val.shape)\nprint(\"X_medium_val:\", X_medium_val.shape)\nprint(\"X_small_val:\", X_small_val.shape)\nprint(\"y_val:\", y_val.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ngpus = tf.config.list_physical_devices('GPU')\nprint(\"GPUs found:\", gpus)\n\n# # Optional: enable mixed precision for speed on modern GPUs\n# mixed_precision.set_global_policy('mixed_float16')\n\n# Use all available GPUs on this machine\nstrategy = tf.distribute.MirroredStrategy()\n\nprint(\"Number of GPUs in sync:\", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input shapes for the model\nlarge_input_shape = (train_image_large_patches_resized.shape[1], train_image_large_patches_resized.shape[2], train_image_large_patches_resized.shape[3])\nmedium_input_shape = (train_image_medium_patches_resized.shape[1], train_image_medium_patches_resized.shape[2], train_image_medium_patches_resized.shape[3])\nsmall_input_shape = (train_image_small_patches_resized.shape[1], train_image_small_patches_resized.shape[2], train_image_small_patches_resized.shape[3])\n\nprint(large_input_shape)\nprint(medium_input_shape)\nprint(small_input_shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ntf.config.optimizer.set_experimental_options({\"layout_optimizer\": False, \"model_pruner\": False})\n\ncheckpoint = ModelCheckpoint(\n    filepath='/kaggle/working/Binary_model_ER_IHC.keras',\n    monitor='val_loss',     \n    save_best_only=True,\n    mode='min',             \n    verbose=1\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',     \n    patience=25,\n    restore_best_weights=True,\n    verbose=1,\n    mode='min'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# Define the VGG16 backbone function\ndef create_vgg16_backbone(input_shape):\n    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    \n    # Make VGG16 layers non-trainable\n#     for layer in vgg16.layers:\n#         layer.trainable = False\n    # Extract feature maps from different layers\n    c1 = vgg16.get_layer('block1_conv2').output  \n    c2 = vgg16.get_layer('block2_conv2').output \n    c3 = vgg16.get_layer('block3_conv3').output  \n\n    return Model(inputs=vgg16.input, outputs=[c1, c2, c3])\n\ndef channel_attention(x):\n    \"\"\"\n    Channel Attention Mechanism.\n    \"\"\"\n    # Global Average Pooling\n    avg_pool = GlobalAveragePooling2D()(x)\n    avg_pool = Reshape((1, 1, x.shape[-1]))(avg_pool)\n    \n    # Fully Connected Layer\n    fc = Dense(x.shape[-1] // 8, activation='relu')(avg_pool)\n    fc = Dense(x.shape[-1], activation='sigmoid')(fc)\n    fc = Reshape((1, 1, x.shape[-1]))(fc)\n    \n    # Multiply with the input\n    attention = Multiply()([x, fc])\n    return attention\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_shape= large_input_shape\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import mixed_precision\nwith strategy.scope():\n    input_shape = target_shape\n\n    large_input = Input(shape=input_shape)\n    medium_input = Input(shape=input_shape)\n    small_input = Input(shape=input_shape)\n\n    vgg16_backbone = create_vgg16_backbone(input_shape)\n\n    # Extract features for each patch size using VGG16\n    large_features = vgg16_backbone(large_input)\n    medium_features = vgg16_backbone(medium_input)\n    small_features = vgg16_backbone(small_input)\n\n    # Concatenate the features from different scales\n    c1 = concatenate([large_features[0], medium_features[0], small_features[0]])\n    c2 = concatenate([large_features[1], medium_features[1], small_features[1]])\n    c3 = concatenate([large_features[2], medium_features[2], small_features[2]])\n  \n   \n    # Apply attention mechanism\n\n    c3 = channel_attention(c3)\n    c2 = channel_attention(c2)\n    c1 = channel_attention(c1)\n\n\n    # Decoder with skip connections and increased filters with dropout\n    x = Conv2DTranspose(1024, (3, 3), strides=(2, 2), padding='same', kernel_regularizer=l2(0.01))(c3)\n    x = concatenate([x, c2])\n    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', kernel_regularizer=l2(0.01))(x)\n    x = concatenate([x, c1])\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    shared = Dropout(0.5)(x)\n\n\n    # Output layer\n    Edge_output = Conv2D(1, kernel_size=1, activation='sigmoid', name='Binary_Edge_Branch')(shared)\n    Segmentation_output = Conv2D(num_classes, 1, activation='sigmoid', name='Segmentation_branch')(shared)\n\n    # Create the final model\n    model = Model(inputs=[large_input, medium_input, small_input], outputs=Segmentation_output)\n    \n    model.compile(\n        optimizer=Adam(),\n        loss='binary_crossentropy',\n        metrics=['accuracy']   \n    )\n    \n\n    # Check the model summary\n    model.summary()\n    total_params = model.count_params()\n    print(f\"Total Trainable Parameters: {total_params / 1e6:.2f}M\")\n\n    # Train the model\n    history = model.fit(\n        [X_large_train, X_medium_train, X_small_train],\n        y_train,\n        validation_data=(\n            [X_large_val, X_medium_val, X_small_val],\n            y_val\n        ),\n        epochs=200,\n        batch_size=16,\n        callbacks=[early_stopping, checkpoint]\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# ---- 1. Extract history ----\nhistory_dict = history.history\n\ntrain_loss = history_dict['loss']\nval_loss   = history_dict['val_loss']\ntrain_acc  = history_dict['accuracy']\nval_acc    = history_dict['val_accuracy']\n\nepochs = range(1, len(train_loss) + 1)\n\n# ---- 2. Plot Loss (this is the most important for segmentation) ----\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, train_loss, label='Training Loss')\nplt.plot(epochs, val_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# ---- 3. Plot Accuracy (secondary, can be misleading for segmentation) ----\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, train_acc, label='Training Accuracy')\nplt.plot(epochs, val_acc, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport time\n\n# ----------------------------------------------------------\n# 1. Select 5 random test images\n# ----------------------------------------------------------\nX_test_large  = train_image_large_patches_resized\nX_test_medium = train_image_medium_patches_resized\nX_test_small  = train_image_small_patches_resized\ny_test        = train_large_mask_cat\n\nnum_samples = len(X_test_large)\nrandom_indices = random.sample(range(num_samples), 5)\n\nprint(\"Selected random test indices:\", random_indices)\n\n# ----------------------------------------------------------\n# Helper Functions\n# ----------------------------------------------------------\n\ndef prepare_rgb(img):\n    \"\"\"Convert to uint8 RGB.\"\"\"\n    img = img.copy()\n    if img.dtype != np.uint8:\n        img = (img * 255).astype(np.uint8)\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ndef colormap_mask(mask):\n    \"\"\"Simple 2-class color mapping.\"\"\"\n    result = np.zeros((*mask.shape, 3), dtype=np.uint8)\n    result[mask == 0] = [40, 0, 60]       # background\n    result[mask == 1] = [255, 235, 50]    # yellow nuclei\n    return result\n\ndef instance_segmentation_watershed(img_rgb, semantic_mask):\n    \"\"\"Return instance-separated contour image using watershed.\"\"\"\n    \n    # 1. Binary mask\n    binary_mask = (semantic_mask > 0).astype(np.uint8)\n\n    # 2. Morphological cleaning\n    kernel = np.ones((3, 3), np.uint8)\n    binary_clean = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n\n    # 3. Distance transform\n    dist = cv2.distanceTransform(binary_clean, cv2.DIST_L2, 5)\n    dist_norm = dist / (dist.max() + 1e-7)\n\n    # 4. Foreground estimation\n    _, sure_fg = cv2.threshold(dist_norm, 0.4, 1.0, cv2.THRESH_BINARY)\n    sure_fg = np.uint8(sure_fg)\n    unknown = cv2.subtract(binary_clean, sure_fg)\n\n    # 5. Connected components â†’ markers\n    num_markers, markers = cv2.connectedComponents(sure_fg)\n    markers = markers + 1\n    markers[unknown == 1] = 0\n\n    # 6. Watershed\n    ws_img = img_rgb.copy()\n    markers = cv2.watershed(ws_img, markers)\n\n    # 7. Draw contours\n    contour_img = img_rgb.copy()\n\n    unique_ids = np.unique(markers)\n    for label in unique_ids:\n        if label <= 1:\n            continue\n        \n        inst_mask = np.uint8(markers == label)\n        cnts, _ = cv2.findContours(inst_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(contour_img, cnts, -1, (255, 0, 0), 2)\n\n    return contour_img\n\n\n# ----------------------------------------------------------\n# 2. Plot 5 Samples\n# ----------------------------------------------------------\nplt.figure(figsize=(22, 20))\n\nfor i, idx in enumerate(random_indices):\n    \n    # ------------------------------\n    # Load image + GT\n    # ------------------------------\n    img_large = X_test_large[idx]\n    img_medium = X_test_medium[idx]\n    img_small = X_test_small[idx]\n    gt_mask = y_test[idx]\n\n    # ------------------------------\n    # Prepare input batch\n    # ------------------------------\n    test_input = [\n        np.expand_dims(img_large, 0),\n        np.expand_dims(img_medium, 0),\n        np.expand_dims(img_small, 0)\n    ]\n\n    # ------------------------------\n    # Predict\n    # ------------------------------\n    pred = model.predict(test_input, verbose=0)\n    pred_probs = pred[0]\n    pred_mask = np.argmax(pred_probs, axis=-1)\n\n    gt_labels = np.argmax(gt_mask, axis=-1)\n\n    # ------------------------------\n    # Prepare images\n    # ------------------------------\n    rgb_img = prepare_rgb(img_large)\n    gt_colored = colormap_mask(gt_labels)\n    pred_colored = colormap_mask(pred_mask)\n\n    # ------------------------------\n    # Instance separation (watershed)\n    # ------------------------------\n    instance_contours_img = instance_segmentation_watershed(rgb_img, pred_mask)\n\n    # ------------------------------\n    # Display row (4 images)\n    # ------------------------------\n    row = i * 4 + 1\n\n    plt.subplot(5, 4, row)\n    plt.imshow(rgb_img)\n    plt.title(f\"Original (idx={idx})\")\n    plt.axis(\"off\")\n\n    plt.subplot(5, 4, row + 1)\n    plt.imshow(gt_colored)\n    plt.title(\"Ground Truth Mask\")\n    plt.axis(\"off\")\n\n    plt.subplot(5, 4, row + 2)\n    plt.imshow(pred_colored)\n    plt.title(\"Predicted Mask\")\n    plt.axis(\"off\")\n\n    plt.subplot(5, 4, row + 3)\n    plt.imshow(instance_contours_img)\n    plt.title(\"Instance Contours\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}